{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7abb8c95-42fe-481f-8cb9-098f60f00c69",
   "metadata": {},
   "source": [
    "# **Hate Speech Detection on Reddit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973202fc-e113-413a-bc60-3a6b6de6fb70",
   "metadata": {},
   "source": [
    "#### To develop a hate speech detection system capable of accurately identifying and classifying hate speech in real-time, ensuring a safer and more welcoming environment for Reddit users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f0838-81e2-495d-8945-2088d11fd0fb",
   "metadata": {},
   "source": [
    "### **Solution Description**\n",
    "##### To tackle hate speech on Reddit, a comprehensive detection system was developed using machine learning (ML) and deep learning (DL) techniques:\n",
    "\n",
    "##### **Data Preparation:** Collected and preprocessed Reddit comments, addressed class imbalances using SMOTE.\n",
    "##### **Model Development:** Implemented and optimized ML models (SVM, SGD Classifier,Random Forest etc.) and DL models (CNN, LSTM).\n",
    "##### **Performance:** LSTM model achieved the best accuracy of 0.84.\n",
    "##### **Prediction:** Using LSTM Model for real-time detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c8295f-1901-4a4c-b3ce-9db567dafc8e",
   "metadata": {},
   "source": [
    "### **Dataset Description:** \n",
    "#### Total Samples: 22,841\n",
    "#### **After Splitting:**\n",
    "#### Training Data, Shape: (17,760 samples, 300 features)\n",
    "##### Class Distribution: {0: 13506 ,1: 4254}\n",
    "#### Testing Data, Shape: (4,440 samples, 300 features)\n",
    "##### Class Distribution: {0: 3408 ,1: 1032}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da18b76-9223-407f-a187-c9f08f182a1b",
   "metadata": {},
   "source": [
    "### Label wise split in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "538ec92a-beb0-457f-b92d-919ce224b183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"label vise train.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"label vise train.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01787f0f-1972-4f03-8c90-f3218a374790",
   "metadata": {},
   "source": [
    "# Best Performing Model: **LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77249733-c966-476b-a903-31f8974bab12",
   "metadata": {},
   "source": [
    "## **Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f58fbcac-4bd8-4b46-9111-97dce7e80920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"classreport.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"classreport.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b386c46-4523-4438-9d7e-033f3a031757",
   "metadata": {},
   "source": [
    "## **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f4f54f5-b453-4bdd-9f27-c99bd79270e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"lstm.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"lstm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e871546d-a425-46f3-935c-f0360e99d79d",
   "metadata": {},
   "source": [
    "# Sample of Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81fe14f5-b8b4-45a7-b313-b4d6e990ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "# Load the FastText model\n",
    "ft_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6909b219-36ce-40fd-872e-12939cd5e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aman Shekhar\n",
      "[nltk_data]     Sachan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aman Shekhar\n",
      "[nltk_data]     Sachan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aman Shekhar\n",
      "[nltk_data]     Sachan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import fasttext  # Ensure fasttext library is installed\n",
    "\n",
    "# Load necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            return \"\"\n",
    "    except:\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    text = ' '.join(expanded_words)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    clean_text = ' '.join(tokens)\n",
    "    return clean_text\n",
    "\n",
    "def preprocess_texts(texts, ft_model):\n",
    "    preprocessed_texts = []\n",
    "    valid_texts = []\n",
    "    for text in texts:\n",
    "        clean_text = preprocess_text(text)\n",
    "        if clean_text == \"\":\n",
    "            continue\n",
    "        words = clean_text.split()\n",
    "        word_vectors = [ft_model.get_word_vector(word) for word in words]\n",
    "        if word_vectors:\n",
    "            text_vector = np.mean(word_vectors, axis=0)\n",
    "            preprocessed_texts.append(text_vector)\n",
    "            valid_texts.append(text)\n",
    "    return np.array(preprocessed_texts), valid_texts\n",
    "\n",
    "def predict_lstm(texts, ft_model, lstm_model):\n",
    "    preprocessed_vectors, valid_texts = preprocess_texts(texts, ft_model)\n",
    "    if preprocessed_vectors.size == 0:\n",
    "        print(\"No valid texts to process.\")\n",
    "        return\n",
    "    \n",
    "    preprocessed_vectors = np.expand_dims(preprocessed_vectors, axis=1)\n",
    "    predictions = lstm_model.predict(preprocessed_vectors)\n",
    "    \n",
    "    for i, text in enumerate(valid_texts):\n",
    "        print(f\"Text: {text}\")\n",
    "        if predictions[i] > 0.5:\n",
    "            print(\"Prediction: Hate Speech\")\n",
    "        else:\n",
    "            print(\"Prediction: Not Hate Speech\")\n",
    "        print()\n",
    "\n",
    "# Load LSTM model\n",
    "lstm_model = load_model('lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c38aba53-84f6-48fe-a269-2be23e239696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter texts (type 'done' when finished):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " You ever fuck a bitch and she start to cry?\n",
      " done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Text: You ever fuck a bitch and she start to cry?\n",
      "Prediction: Hate Speech\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# input from the user\n",
    "texts = []\n",
    "print(\"Enter texts (type 'done' when finished):\")\n",
    "while True:\n",
    "    user_input = input()\n",
    "    if user_input.lower() == 'done':\n",
    "        break\n",
    "    texts.append(user_input)\n",
    "\n",
    "# Call the prediction function\n",
    "predict_lstm(texts, ft_model, lstm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f50554-3e26-4472-bc7c-6c00bbc7f178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
