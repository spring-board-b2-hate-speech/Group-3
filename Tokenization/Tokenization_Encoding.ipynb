{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c510c9e1-f768-4c87-8f29-5ea1a117bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a888dcae-62a4-43de-a95d-03386892a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>hate_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A subsection of retarded Hungarians? Oh boy. b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iii. Just got off work. Foundation and groundi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow i guess cowboys are the same in every country</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owen Benjamin's cowboy song goes for every cou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt; \"y'all hear sun?\" by all means I live in a s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  hate_speech\n",
       "0  A subsection of retarded Hungarians? Oh boy. b...            1\n",
       "1  Iii. Just got off work. Foundation and groundi...            0\n",
       "2  wow i guess cowboys are the same in every country            0\n",
       "3  Owen Benjamin's cowboy song goes for every cou...            0\n",
       "4  > \"y'all hear sun?\" by all means I live in a s...            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Reddit_final.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfafc0-960d-4041-ac0a-806dc1facddc",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ac75a1-1edf-4e79-a135-3771186461e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aman Shekhar\n",
      "[nltk_data]     Sachan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  \\\n",
      "0      A subsection of retarded Hungarians? Oh boy. b...   \n",
      "1      Iii. Just got off work. Foundation and groundi...   \n",
      "2      wow i guess cowboys are the same in every country   \n",
      "3      Owen Benjamin's cowboy song goes for every cou...   \n",
      "4      > \"y'all hear sun?\" by all means I live in a s...   \n",
      "...                                                  ...   \n",
      "22241  Of, stop being a forgot and post videos next t...   \n",
      "22242  In this minute long video, Top Hate and Champa...   \n",
      "22243  No clue whos these e-celebs are, but at this p...   \n",
      "22244      I didn’t insult you, why would you insult me?   \n",
      "22245                      Because you are living a lie.   \n",
      "\n",
      "                                                  tokens  \n",
      "0      [a, subsection, of, retarded, hungarians, ?, o...  \n",
      "1      [iii, ., just, got, off, work, ., foundation, ...  \n",
      "2      [wow, i, guess, cowboys, are, the, same, in, e...  \n",
      "3      [owen, benjamin, 's, cowboy, song, goes, for, ...  \n",
      "4      [>, ``, y'all, hear, sun, ?, '', by, all, mean...  \n",
      "...                                                  ...  \n",
      "22241  [of, ,, stop, being, a, forgot, and, post, vid...  \n",
      "22242  [in, this, minute, long, video, ,, top, hate, ...  \n",
      "22243  [no, clue, whos, these, e-celebs, are, ,, but,...  \n",
      "22244  [i, didn, ’, t, insult, you, ,, why, would, yo...  \n",
      "22245             [because, you, are, living, a, lie, .]  \n",
      "\n",
      "[22246 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization with NLTK - Handles punctuation and contractions and Suitable for general text processing.\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        return word_tokenize(text.lower())\n",
    "    else:\n",
    "        return []  # Return an empty list for NaN or non-string inputs\n",
    "\n",
    "\n",
    "# Apply tokenization to the DataFrame\n",
    "df['tokens'] = df['comment'].apply(tokenize_text)\n",
    "print(df[['comment', 'tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19172b4-9d87-4cb4-8aaa-9b1f8262c198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 comment  \\\n",
      "0      A subsection of retarded Hungarians? Oh boy. b...   \n",
      "1      Iii. Just got off work. Foundation and groundi...   \n",
      "2      wow i guess cowboys are the same in every country   \n",
      "3      Owen Benjamin's cowboy song goes for every cou...   \n",
      "4      > \"y'all hear sun?\" by all means I live in a s...   \n",
      "...                                                  ...   \n",
      "22241  Of, stop being a forgot and post videos next t...   \n",
      "22242  In this minute long video, Top Hate and Champa...   \n",
      "22243  No clue whos these e-celebs are, but at this p...   \n",
      "22244      I didn’t insult you, why would you insult me?   \n",
      "22245                      Because you are living a lie.   \n",
      "\n",
      "                                            tokens_spacy  \n",
      "0      [a, subsection, of, retarded, hungarians, ?, o...  \n",
      "1      [iii, ., just, got, off, work, ., foundation, ...  \n",
      "2      [wow, i, guess, cowboys, are, the, same, in, e...  \n",
      "3      [owen, benjamin, 's, cowboy, song, goes, for, ...  \n",
      "4      [>, \", y', all, hear, sun, ?, \", by, all, mean...  \n",
      "...                                                  ...  \n",
      "22241  [of, ,, stop, being, a, forgot, and, post, vid...  \n",
      "22242  [in, this, minute, long, video, ,, top, hate, ...  \n",
      "22243  [no, clue, who, s, these, e, -, celebs, are, ,...  \n",
      "22244  [i, did, n’t, insult, you, ,, why, would, you,...  \n",
      "22245             [because, you, are, living, a, lie, .]  \n",
      "\n",
      "[22246 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#SpaCy Tokenization - A robust tokenizer that handles punctuation, contractions, and multi-word expressions.\n",
    "#(Handles a wide variety of text and Good for syntactic and semantic analysis.)\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "df['tokens_spacy'] = df['comment'].apply(lambda x: [token.text.lower() for token in nlp(x)])\n",
    "print(df[['comment', 'tokens_spacy']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6fe8d5-ad43-45e0-812e-833f40068852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A subsection of retarded Hungarians? Oh boy. b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, subsection, of, retarded, hungarians, ?, o...</td>\n",
       "      <td>[a, subsection, of, retarded, hungarians, ?, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iii. Just got off work. Foundation and groundi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[iii, ., just, got, off, work, ., foundation, ...</td>\n",
       "      <td>[iii, ., just, got, off, work, ., foundation, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow i guess cowboys are the same in every country</td>\n",
       "      <td>0</td>\n",
       "      <td>[wow, i, guess, cowboys, are, the, same, in, e...</td>\n",
       "      <td>[wow, i, guess, cowboys, are, the, same, in, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owen Benjamin's cowboy song goes for every cou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[owen, benjamin, 's, cowboy, song, goes, for, ...</td>\n",
       "      <td>[owen, benjamin, 's, cowboy, song, goes, for, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt; \"y'all hear sun?\" by all means I live in a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&gt;, ``, y'all, hear, sun, ?, '', by, all, mean...</td>\n",
       "      <td>[&gt;, \", y', all, hear, sun, ?, \", by, all, mean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  hate_speech  \\\n",
       "0  A subsection of retarded Hungarians? Oh boy. b...            1   \n",
       "1  Iii. Just got off work. Foundation and groundi...            0   \n",
       "2  wow i guess cowboys are the same in every country            0   \n",
       "3  Owen Benjamin's cowboy song goes for every cou...            0   \n",
       "4  > \"y'all hear sun?\" by all means I live in a s...            0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [a, subsection, of, retarded, hungarians, ?, o...   \n",
       "1  [iii, ., just, got, off, work, ., foundation, ...   \n",
       "2  [wow, i, guess, cowboys, are, the, same, in, e...   \n",
       "3  [owen, benjamin, 's, cowboy, song, goes, for, ...   \n",
       "4  [>, ``, y'all, hear, sun, ?, '', by, all, mean...   \n",
       "\n",
       "                                        tokens_spacy  \n",
       "0  [a, subsection, of, retarded, hungarians, ?, o...  \n",
       "1  [iii, ., just, got, off, work, ., foundation, ...  \n",
       "2  [wow, i, guess, cowboys, are, the, same, in, e...  \n",
       "3  [owen, benjamin, 's, cowboy, song, goes, for, ...  \n",
       "4  [>, \", y', all, hear, sun, ?, \", by, all, mean...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26077e2-e17b-449a-ac30-0bc27bb56023",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36c13e8d-9ca5-43ac-a78f-2afd6b7ed06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0  0.363497 -0.220945  0.010517 -0.602767  0.172479 -0.504409  0.794537   \n",
      "1  0.065042 -0.122197  0.100385 -0.005709  0.070777 -0.620618  0.681243   \n",
      "2  0.292166  0.385734 -0.048033 -0.141083  0.212870 -0.512191  0.579481   \n",
      "3  0.074742  0.083152 -0.081609 -0.397689 -0.137089 -1.063069  0.531439   \n",
      "4  0.192634 -0.107493 -0.089676 -0.271427  0.183969 -0.576812  0.581196   \n",
      "\n",
      "         7         8         9   ...        90        91        92        93  \\\n",
      "0  0.956250  0.095910 -0.555402  ...  0.367883  0.511329  0.360923  0.054399   \n",
      "1  1.025234 -0.097676 -0.647134  ...  0.431777  0.739346  0.304107  0.122711   \n",
      "2  0.932011 -0.035010 -0.343719  ...  0.612062  1.063374 -0.273927 -0.392330   \n",
      "3  0.918282 -0.469783 -0.672046  ...  0.474360  0.966216  0.182148 -0.093319   \n",
      "4  0.974797 -0.254152 -0.619714  ...  0.617659  0.673389 -0.008742  0.090960   \n",
      "\n",
      "         94        95        96        97        98        99  \n",
      "0  0.543700  0.174107 -0.015235 -0.560962  0.362332 -0.059251  \n",
      "1  0.545432  0.287484 -0.383720 -0.303876 -0.060889 -0.334727  \n",
      "2  0.681582  0.383872  0.114081  0.072461  0.102290 -0.145542  \n",
      "3  0.417795  0.220918 -0.059033 -0.544366 -0.148031  0.025088  \n",
      "4  0.473399  0.386444 -0.162406 -0.359978  0.112415 -0.089078  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to get Word2Vec vectors for each text\n",
    "def get_word2vec_vectors(tokens, model, vector_size):\n",
    "    vector = np.zeros(vector_size)\n",
    "    valid_words = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            vector += model.wv[word]\n",
    "            valid_words += 1\n",
    "    if valid_words > 0:\n",
    "        vector /= valid_words\n",
    "    return vector\n",
    "\n",
    "# Apply Word2Vec to the dataset\n",
    "df['word2vec_vector'] = df['tokens'].apply(lambda x: get_word2vec_vectors(x, word2vec_model, 100))\n",
    "\n",
    "# Convert Word2Vec features to a DataFrame\n",
    "word2vec_df = pd.DataFrame(df['word2vec_vector'].tolist())\n",
    "\n",
    "print(word2vec_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "983bd5f2-4e9b-45ed-8a8d-46696ecd4f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_spacy</th>\n",
       "      <th>word2vec_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A subsection of retarded Hungarians? Oh boy. b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, subsection, of, retarded, hungarians, ?, o...</td>\n",
       "      <td>[a, subsection, of, retarded, hungarians, ?, o...</td>\n",
       "      <td>[0.3634971592671655, -0.2209453577143622, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iii. Just got off work. Foundation and groundi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[iii, ., just, got, off, work, ., foundation, ...</td>\n",
       "      <td>[iii, ., just, got, off, work, ., foundation, ...</td>\n",
       "      <td>[0.06504157067821273, -0.1221966900651888, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow i guess cowboys are the same in every country</td>\n",
       "      <td>0</td>\n",
       "      <td>[wow, i, guess, cowboys, are, the, same, in, e...</td>\n",
       "      <td>[wow, i, guess, cowboys, are, the, same, in, e...</td>\n",
       "      <td>[0.29216628866270183, 0.3857342477887869, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owen Benjamin's cowboy song goes for every cou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[owen, benjamin, 's, cowboy, song, goes, for, ...</td>\n",
       "      <td>[owen, benjamin, 's, cowboy, song, goes, for, ...</td>\n",
       "      <td>[0.07474208818489893, 0.08315160974032348, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt; \"y'all hear sun?\" by all means I live in a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&gt;, ``, y'all, hear, sun, ?, '', by, all, mean...</td>\n",
       "      <td>[&gt;, \", y', all, hear, sun, ?, \", by, all, mean...</td>\n",
       "      <td>[0.19263355600529572, -0.10749349491605141, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  hate_speech  \\\n",
       "0  A subsection of retarded Hungarians? Oh boy. b...            1   \n",
       "1  Iii. Just got off work. Foundation and groundi...            0   \n",
       "2  wow i guess cowboys are the same in every country            0   \n",
       "3  Owen Benjamin's cowboy song goes for every cou...            0   \n",
       "4  > \"y'all hear sun?\" by all means I live in a s...            0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [a, subsection, of, retarded, hungarians, ?, o...   \n",
       "1  [iii, ., just, got, off, work, ., foundation, ...   \n",
       "2  [wow, i, guess, cowboys, are, the, same, in, e...   \n",
       "3  [owen, benjamin, 's, cowboy, song, goes, for, ...   \n",
       "4  [>, ``, y'all, hear, sun, ?, '', by, all, mean...   \n",
       "\n",
       "                                        tokens_spacy  \\\n",
       "0  [a, subsection, of, retarded, hungarians, ?, o...   \n",
       "1  [iii, ., just, got, off, work, ., foundation, ...   \n",
       "2  [wow, i, guess, cowboys, are, the, same, in, e...   \n",
       "3  [owen, benjamin, 's, cowboy, song, goes, for, ...   \n",
       "4  [>, \", y', all, hear, sun, ?, \", by, all, mean...   \n",
       "\n",
       "                                     word2vec_vector  \n",
       "0  [0.3634971592671655, -0.2209453577143622, 0.01...  \n",
       "1  [0.06504157067821273, -0.1221966900651888, 0.1...  \n",
       "2  [0.29216628866270183, 0.3857342477887869, -0.0...  \n",
       "3  [0.07474208818489893, 0.08315160974032348, -0....  \n",
       "4  [0.19263355600529572, -0.10749349491605141, -0...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13976aa7-4f7c-4bbd-9f13-10c45c2a77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import fasttext\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_comment'] = df['comment'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c9faef-3129-4d02-8b86-10893d89e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FastText model\n",
    "ft_model = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fbc8501-7a6f-45e5-8425-a6f89f6040db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get FastText vector\n",
    "def get_fasttext_vector(text, model):\n",
    "    words = text.split()\n",
    "    vector = model.get_sentence_vector(' '.join(words))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09685147-8d04-499d-92bd-97f4b6152662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_comment  \\\n",
      "0      a subsection of retarded hungarians oh boy bra...   \n",
      "1      iii just got off work foundation and grounding...   \n",
      "2      wow i guess cowboys are the same in every country   \n",
      "3      owen benjamins cowboy song goes for every coun...   \n",
      "4      yall hear sun by all means i live in a small t...   \n",
      "...                                                  ...   \n",
      "22241  of stop being a forgot and post videos next ti...   \n",
      "22242  in this minute long video top hate and champag...   \n",
      "22243  no clue whos these ecelebs are but at this poi...   \n",
      "22244         i didnt insult you why would you insult me   \n",
      "22245                       because you are living a lie   \n",
      "\n",
      "                                         fasttext_vector  \n",
      "0      [0.004770048428326845, -0.03953176364302635, -...  \n",
      "1      [-0.012044071219861507, -0.011684201657772064,...  \n",
      "2      [-0.0008766286191530526, 0.0192383024841547, 0...  \n",
      "3      [0.00418263953179121, -0.0029789397958666086, ...  \n",
      "4      [-6.847345503047109e-05, 0.0004209601611364633...  \n",
      "...                                                  ...  \n",
      "22241  [0.010218881070613861, 0.015606718137860298, 0...  \n",
      "22242  [0.004358708392828703, -0.006413696799427271, ...  \n",
      "22243  [-0.0025172580499202013, -0.005129380617290735...  \n",
      "22244  [0.02669641375541687, -0.015254346653819084, 0...  \n",
      "22245  [-0.011769361793994904, -0.024160273373126984,...  \n",
      "\n",
      "[22246 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply FastText encoding\n",
    "df['fasttext_vector'] = df['cleaned_comment'].apply(lambda x: get_fasttext_vector(x, ft_model))\n",
    "df['fasttext_vector'] = df['fasttext_vector'].apply(lambda x: x.tolist())\n",
    "\n",
    "# Display the DataFrame with FastText vectors\n",
    "print(df[['cleaned_comment', 'fasttext_vector']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13a13248-1164-492a-9be9-25b45828b94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_spacy</th>\n",
       "      <th>word2vec_vector</th>\n",
       "      <th>cleaned_comment</th>\n",
       "      <th>fasttext_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A subsection of retarded Hungarians? Oh boy. b...</td>\n",
       "      <td>1</td>\n",
       "      <td>[a, subsection, of, retarded, hungarians, ?, o...</td>\n",
       "      <td>[a, subsection, of, retarded, hungarians, ?, o...</td>\n",
       "      <td>[0.3634971592671655, -0.2209453577143622, 0.01...</td>\n",
       "      <td>a subsection of retarded hungarians oh boy bra...</td>\n",
       "      <td>[0.004770048428326845, -0.03953176364302635, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iii. Just got off work. Foundation and groundi...</td>\n",
       "      <td>0</td>\n",
       "      <td>[iii, ., just, got, off, work, ., foundation, ...</td>\n",
       "      <td>[iii, ., just, got, off, work, ., foundation, ...</td>\n",
       "      <td>[0.06504157067821273, -0.1221966900651888, 0.1...</td>\n",
       "      <td>iii just got off work foundation and grounding...</td>\n",
       "      <td>[-0.012044071219861507, -0.011684201657772064,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wow i guess cowboys are the same in every country</td>\n",
       "      <td>0</td>\n",
       "      <td>[wow, i, guess, cowboys, are, the, same, in, e...</td>\n",
       "      <td>[wow, i, guess, cowboys, are, the, same, in, e...</td>\n",
       "      <td>[0.29216628866270183, 0.3857342477887869, -0.0...</td>\n",
       "      <td>wow i guess cowboys are the same in every country</td>\n",
       "      <td>[-0.0008766286191530526, 0.0192383024841547, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owen Benjamin's cowboy song goes for every cou...</td>\n",
       "      <td>0</td>\n",
       "      <td>[owen, benjamin, 's, cowboy, song, goes, for, ...</td>\n",
       "      <td>[owen, benjamin, 's, cowboy, song, goes, for, ...</td>\n",
       "      <td>[0.07474208818489893, 0.08315160974032348, -0....</td>\n",
       "      <td>owen benjamins cowboy song goes for every coun...</td>\n",
       "      <td>[0.00418263953179121, -0.0029789397958666086, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt; \"y'all hear sun?\" by all means I live in a s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[&gt;, ``, y'all, hear, sun, ?, '', by, all, mean...</td>\n",
       "      <td>[&gt;, \", y', all, hear, sun, ?, \", by, all, mean...</td>\n",
       "      <td>[0.19263355600529572, -0.10749349491605141, -0...</td>\n",
       "      <td>yall hear sun by all means i live in a small t...</td>\n",
       "      <td>[-6.847345503047109e-05, 0.0004209601611364633...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22241</th>\n",
       "      <td>Of, stop being a forgot and post videos next t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[of, ,, stop, being, a, forgot, and, post, vid...</td>\n",
       "      <td>[of, ,, stop, being, a, forgot, and, post, vid...</td>\n",
       "      <td>[0.18222494916442564, -0.2874639831921634, 0.0...</td>\n",
       "      <td>of stop being a forgot and post videos next ti...</td>\n",
       "      <td>[0.010218881070613861, 0.015606718137860298, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22242</th>\n",
       "      <td>In this minute long video, Top Hate and Champa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[in, this, minute, long, video, ,, top, hate, ...</td>\n",
       "      <td>[in, this, minute, long, video, ,, top, hate, ...</td>\n",
       "      <td>[0.0843904949122526, 0.06925528072591486, -0.0...</td>\n",
       "      <td>in this minute long video top hate and champag...</td>\n",
       "      <td>[0.004358708392828703, -0.006413696799427271, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22243</th>\n",
       "      <td>No clue whos these e-celebs are, but at this p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[no, clue, whos, these, e-celebs, are, ,, but,...</td>\n",
       "      <td>[no, clue, who, s, these, e, -, celebs, are, ,...</td>\n",
       "      <td>[0.26610068939683007, -0.041094351843817205, -...</td>\n",
       "      <td>no clue whos these ecelebs are but at this poi...</td>\n",
       "      <td>[-0.0025172580499202013, -0.005129380617290735...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22244</th>\n",
       "      <td>I didn’t insult you, why would you insult me?</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, didn, ’, t, insult, you, ,, why, would, yo...</td>\n",
       "      <td>[i, did, n’t, insult, you, ,, why, would, you,...</td>\n",
       "      <td>[0.706557840681993, 0.4792756805053124, -0.129...</td>\n",
       "      <td>i didnt insult you why would you insult me</td>\n",
       "      <td>[0.02669641375541687, -0.015254346653819084, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22245</th>\n",
       "      <td>Because you are living a lie.</td>\n",
       "      <td>0</td>\n",
       "      <td>[because, you, are, living, a, lie, .]</td>\n",
       "      <td>[because, you, are, living, a, lie, .]</td>\n",
       "      <td>[0.15619449916162662, -0.6708248012832233, -0....</td>\n",
       "      <td>because you are living a lie</td>\n",
       "      <td>[-0.011769361793994904, -0.024160273373126984,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22246 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  hate_speech  \\\n",
       "0      A subsection of retarded Hungarians? Oh boy. b...            1   \n",
       "1      Iii. Just got off work. Foundation and groundi...            0   \n",
       "2      wow i guess cowboys are the same in every country            0   \n",
       "3      Owen Benjamin's cowboy song goes for every cou...            0   \n",
       "4      > \"y'all hear sun?\" by all means I live in a s...            0   \n",
       "...                                                  ...          ...   \n",
       "22241  Of, stop being a forgot and post videos next t...            1   \n",
       "22242  In this minute long video, Top Hate and Champa...            0   \n",
       "22243  No clue whos these e-celebs are, but at this p...            1   \n",
       "22244      I didn’t insult you, why would you insult me?            0   \n",
       "22245                      Because you are living a lie.            0   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [a, subsection, of, retarded, hungarians, ?, o...   \n",
       "1      [iii, ., just, got, off, work, ., foundation, ...   \n",
       "2      [wow, i, guess, cowboys, are, the, same, in, e...   \n",
       "3      [owen, benjamin, 's, cowboy, song, goes, for, ...   \n",
       "4      [>, ``, y'all, hear, sun, ?, '', by, all, mean...   \n",
       "...                                                  ...   \n",
       "22241  [of, ,, stop, being, a, forgot, and, post, vid...   \n",
       "22242  [in, this, minute, long, video, ,, top, hate, ...   \n",
       "22243  [no, clue, whos, these, e-celebs, are, ,, but,...   \n",
       "22244  [i, didn, ’, t, insult, you, ,, why, would, yo...   \n",
       "22245             [because, you, are, living, a, lie, .]   \n",
       "\n",
       "                                            tokens_spacy  \\\n",
       "0      [a, subsection, of, retarded, hungarians, ?, o...   \n",
       "1      [iii, ., just, got, off, work, ., foundation, ...   \n",
       "2      [wow, i, guess, cowboys, are, the, same, in, e...   \n",
       "3      [owen, benjamin, 's, cowboy, song, goes, for, ...   \n",
       "4      [>, \", y', all, hear, sun, ?, \", by, all, mean...   \n",
       "...                                                  ...   \n",
       "22241  [of, ,, stop, being, a, forgot, and, post, vid...   \n",
       "22242  [in, this, minute, long, video, ,, top, hate, ...   \n",
       "22243  [no, clue, who, s, these, e, -, celebs, are, ,...   \n",
       "22244  [i, did, n’t, insult, you, ,, why, would, you,...   \n",
       "22245             [because, you, are, living, a, lie, .]   \n",
       "\n",
       "                                         word2vec_vector  \\\n",
       "0      [0.3634971592671655, -0.2209453577143622, 0.01...   \n",
       "1      [0.06504157067821273, -0.1221966900651888, 0.1...   \n",
       "2      [0.29216628866270183, 0.3857342477887869, -0.0...   \n",
       "3      [0.07474208818489893, 0.08315160974032348, -0....   \n",
       "4      [0.19263355600529572, -0.10749349491605141, -0...   \n",
       "...                                                  ...   \n",
       "22241  [0.18222494916442564, -0.2874639831921634, 0.0...   \n",
       "22242  [0.0843904949122526, 0.06925528072591486, -0.0...   \n",
       "22243  [0.26610068939683007, -0.041094351843817205, -...   \n",
       "22244  [0.706557840681993, 0.4792756805053124, -0.129...   \n",
       "22245  [0.15619449916162662, -0.6708248012832233, -0....   \n",
       "\n",
       "                                         cleaned_comment  \\\n",
       "0      a subsection of retarded hungarians oh boy bra...   \n",
       "1      iii just got off work foundation and grounding...   \n",
       "2      wow i guess cowboys are the same in every country   \n",
       "3      owen benjamins cowboy song goes for every coun...   \n",
       "4      yall hear sun by all means i live in a small t...   \n",
       "...                                                  ...   \n",
       "22241  of stop being a forgot and post videos next ti...   \n",
       "22242  in this minute long video top hate and champag...   \n",
       "22243  no clue whos these ecelebs are but at this poi...   \n",
       "22244         i didnt insult you why would you insult me   \n",
       "22245                       because you are living a lie   \n",
       "\n",
       "                                         fasttext_vector  \n",
       "0      [0.004770048428326845, -0.03953176364302635, -...  \n",
       "1      [-0.012044071219861507, -0.011684201657772064,...  \n",
       "2      [-0.0008766286191530526, 0.0192383024841547, 0...  \n",
       "3      [0.00418263953179121, -0.0029789397958666086, ...  \n",
       "4      [-6.847345503047109e-05, 0.0004209601611364633...  \n",
       "...                                                  ...  \n",
       "22241  [0.010218881070613861, 0.015606718137860298, 0...  \n",
       "22242  [0.004358708392828703, -0.006413696799427271, ...  \n",
       "22243  [-0.0025172580499202013, -0.005129380617290735...  \n",
       "22244  [0.02669641375541687, -0.015254346653819084, 0...  \n",
       "22245  [-0.011769361793994904, -0.024160273373126984,...  \n",
       "\n",
       "[22246 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81967028-05f5-4902-9b22-75e3a70fed45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('RedditTokenized.csv', index=False)\n",
    "print(\"File saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
